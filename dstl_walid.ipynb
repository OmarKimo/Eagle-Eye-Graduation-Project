{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"dstl_walid.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"F-XGfxBP131-","colab_type":"text"},"source":["# Start From Here"]},{"cell_type":"code","metadata":{"id":"gcHD4lvkYWjq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"executionInfo":{"status":"ok","timestamp":1596789659525,"user_tz":-120,"elapsed":30634,"user":{"displayName":"Walid Mohammad Hella","photoUrl":"https://lh3.googleusercontent.com/-MwgtfecJDio/AAAAAAAAAAI/AAAAAAAAAHo/wusiBvv1oRc/s64/photo.jpg","userId":"04532010003537326230"}},"outputId":"f99b8d82-6917-4d39-de86-73842c1297cb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d6RcFo4pnoDm","colab_type":"code","colab":{}},"source":["import numpy as np\n","import math"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sbn75oTCYJt","colab_type":"code","colab":{}},"source":["import pandas as pd\n","tr_wkt = pd.read_csv('/content/drive/My Drive/GP/dstl/train_wkt_v4.csv')  # Well-known text Multipolygon\n","grid_sizes = pd.read_csv('/content/drive/My Drive/GP/dstl/grid_sizes.csv', names=['ImageId', 'Xmax', 'Ymin'], skiprows=1)\n","\n","def convert_coordinates(coords, img_size, xy_coords):\n","    '''\n","    Convert all image coordinates given range\n","    '''\n","    # https://www.kaggle.com/visoft/dstl-satellite-imagery-feature-detection/export-pixel-wise-mask\n","    Xmax, Ymin = xy_coords              # Xmax - maximum X coordinate for the image\n","                                        # Ymin - minimum Y coordinate for the image\n","    Height, Weight = img_size            # image sizes\n","    W = 1.0 * Weight * Weight / (Weight + 1)\n","    H = 1.0 * Height * Height / (Height + 1)\n","    xf = W / Xmax\n","    yf = H / Ymin\n","    coords[:, 1] *= yf\n","    coords[:, 0] *= xf\n","    coords_int = np.round(coords).astype(np.int32)\n","    return coords_int\n","def get_xmax_ymin(GS, imageId):\n","    '''\n","    Find maximum coordinates for x and minimun coordinates for y\n","    ''' \n","    xmax, ymin = GS[GS.ImageId == imageId].iloc[0, 1:].astype(float)\n","    return (xmax, ymin)\n","def get_polygon_list(wkt_list, imageId, cType):\n","    '''\n","    Find polygon list for all images with there class label\n","    '''\n","    # https://www.kaggle.com/visoft/dstl-satellite-imagery-feature-detection/export-pixel-wise-mask\n","    df_image = wkt_list[wkt_list.ImageId == imageId]\n","    multipoly_def = df_image[df_image.ClassType == cType].MultipolygonWKT\n","    polygonList = None\n","    if len(multipoly_def) > 0:\n","        assert len(multipoly_def) == 1\n","        polygonList = loads(multipoly_def.values[0])\n","    return polygonList\n","def get_and_convert_contours(polygonList, img_size, xy_coods):\n","    # https://www.kaggle.com/visoft/dstl-satellite-imagery-feature-detection/export-pixel-wise-mask\n","    perim_list = []\n","    interior_list = []\n","    if polygonList is None:\n","        return None\n","    for k in range(len(polygonList)):\n","        poly = polygonList[k]\n","        perim = np.array(list(poly.exterior.coords))\n","        perim_c = convert_coordinates(perim, img_size, xy_coods)\n","        perim_list.append(perim_c)\n","        for pi in poly.interiors:\n","            interior = np.array(list(pi.coords))\n","            interior_c = convert_coordinates(interior, img_size, xy_coods)\n","            interior_list.append(interior_c)\n","    return perim_list, interior_list\n","def plot_mask_from_contours(img_size, contours):\n","    # https://www.kaggle.com/visoft/dstl-satellite-imagery-feature-detection/export-pixel-wise-mask\n","    img_mask = np.zeros(img_size, np.uint8)\n","    if contours is None:\n","        return img_mask\n","    perim_list, interior_list = contours\n","    cv2.fillPoly(img_mask, perim_list,1)    \n","    cv2.fillPoly(img_mask, interior_list,0)\n","    return img_mask\n","def generate_mask_for_image_and_class(img_size, imageId, class_type, GS=grid_sizes, wkt_list=tr_wkt):\n","    '''\n","    Fill polygons exterior and interior points and return mask of images\n","    '''\n","    xy_coods = get_xmax_ymin(GS, imageId)\n","    polygon_list = get_polygon_list(wkt_list, imageId, class_type)\n","    contours = get_and_convert_contours(polygon_list, img_size, xy_coods)\n","    mask = plot_mask_from_contours(img_size, contours)\n","    return mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZT5977c95AP","colab_type":"code","colab":{}},"source":["def computeNormal2D(mat): #input must be a 2D array\n","    amin = np.amin(mat)\n","    mat = (mat - amin) / (np.amax(mat) - amin)\n","    return mat\n","\n","def computeNormal3D(mat): #input must be a 3D array\n","    for i in range(mat.shape[2]):\n","        mat[:,:,i] = computeNormal2D(mat[:,:,i])\n","    return mat\n","\n","def computeSMoment(mat,mean=None): #input must be a 2D array\n","  if mean == None:\n","    mean = mat.mean()\n","  return math.sqrt(mean)/np.square(mat).mean()\n","\n","def convertRGB2HSI(img):\n","    img = img/1023.0\n","    imgHSI = img\n","    for i in range(len(img)):\n","        for j in range(len(img[0])):\n","            cmax = max(img[i][j][0],img[i][j][1],img[i][j][2])\n","            cmin = min(img[i][j][0],img[i][j][1],img[i][j][2])\n","            delta = cmax-cmin\n","\n","            if cmax==cmin:\n","                imgHSI[i][j][0] = 0\n","            elif cmax == img[i][j][0]:\n","                imgHSI[i][j][0] = (60 * ((img[i][j][1] - img[i][j][2]) / delta) + 360) % 360\n","            elif cmax == img[i][j][1]:\n","                imgHSI[i][j][0] = (60 * ((img[i][j][2] - img[i][j][0]) / delta) + 120) % 360\n","            elif cmax == img[i][j][2]:\n","                imgHSI[i][j][0] = (60 * ((img[i][j][0] - img[i][j][1]) / delta) + 240) % 360\n","      \n","            if cmax==0:\n","                imgHSI[i][j][1]=0\n","            else:\n","                imgHSI[i][j][1]=(delta/cmax)\n","      \n","            imgHSI[i][j][2]=cmax\n","\n","    return imgHSI\n","\n","def computeCCMFeatures(img,band,windowSize = 7):\n","\n","    if windowSize % 2 == 0:\n","        return\n","    paramCount = 0\n","    if band == 'H':\n","        paramCount = 3\n","        img = img*255/360\n","    elif band == 'S':\n","        paramCount = 2\n","        img = img*255\n","    elif band == 'I':\n","        paramCount = 2\n","        img = img*255\n","    else:\n","        return\n","  \n","    img = np.around(img)  \n","    img = img.astype(np.uint8)\n","  \n","    newImg = np.zeros((478,478,paramCount))\n","    halfWindow=windowSize//2\n","    ccm = np.zeros((256,256), dtype=np.uint8) #assuming windowSize <= 15\n","    a = 0\n","    b = 0\n","    for i in range(halfWindow,img.shape[0]-halfWindow,windowSize):\n","        for j in range(halfWindow,img.shape[1]-halfWindow,windowSize):\n","            # i,j = pixel in whole-image loop\n","            ccm = np.zeros((256,256), dtype=np.uint8) #assuming windowSize <= 15\n","            for k in range(i-halfWindow,min(img.shape[0]-1,i+halfWindow+1)):\n","                for l in range(j-halfWindow,min(img.shape[1]-1,j+halfWindow+1)):\n","                    # k,l = pixel in sliding window loop\n","                    ccm[img[k,l],img[k+1,l+1]] += 1\n","            mean = ccm.mean()\n","            newImg[a][b][0] = mean\n","            ccmSquared = np.square(ccm)\n","        \n","            if band == 'H':\n","                #sosvh is SSD\n","                window= img[(i-halfWindow):min(img.shape[0]-1,i+halfWindow+1), (j-halfWindow):min(img.shape[1]-1,j+halfWindow+1)]\n","                newImg[a][b][1]=np.sum(np.square(window-mean)) #sosvh\n","                newImg[a][b][2]=np.sum(ccmSquared) #autoc\n","            else:\n","                newImg[a][b][1]=(math.sqrt(mean))/(ccmSquared.mean()) #smoment\n","                #if band == 'I':\n","                #  covariance = np.cov(ccm)[0][-1] ###### Check if it works for a matrix and replace with my function\n","                #  newImg[a][b][2]=covariance\n","            b = (b+1)%478\n","        a += 1\n","    return newImg\n","\n","\n","def computeHSIFeatures(img,windowSize = 7):\n","    paramCount = 7\n","    newImg = np.zeros((478,478,paramCount))\n","    halfWindow=windowSize//2\n","    a = 0\n","    b = 0\n","    for i in range(halfWindow,img.shape[0]-halfWindow,7):\n","        for j in range(halfWindow,img.shape[1]-halfWindow,7):\n","            # i,j = pixel in whole-image loop      \n","        \n","            minX=i-halfWindow\n","            maxX=i+halfWindow+1\n","            minY=j-halfWindow\n","            maxY=j+halfWindow+1\n","\n","            newImg[a][b][4] = img[minX:maxX,minY:maxY,0].mean() #meanH\n","            newImg[a][b][6] = img[minX:maxX,minY:maxY,1].mean() #meanS\n","            newImg[a][b][5] = img[minX:maxX,minY:maxY,2].mean() #meanI\n","            newImg[a][b][0] = computeSMoment(img[minX:maxX,minY:maxY,2], newImg[a][b][5]) #smomentI\n","            newImg[a][b][1] = np.var(img[minX:maxX,minY:maxY,2]) #varianceI\n","            newImg[a][b][2] = math.sqrt(newImg[a][b][1]) #stdI\n","            newImg[a][b][3] = np.std(img[minX:maxX,minY:maxY,0]) #stdH\n","            b = (b+1)%478\n","        a += 1\n","        \n","    return newImg\n","\n","def computeRGBNFeatures(img,windowSize = 7):\n","    paramCount = 2\n","    newImg = np.zeros((478,478,paramCount))\n","    halfWindow=windowSize//2\n","    a = 0\n","    b = 0\n","    for i in range(halfWindow,img.shape[0]-halfWindow,7):\n","        for j in range(halfWindow,img.shape[1]-halfWindow,7):\n","            # i,j = pixel in whole-image loop      \n","            \n","            minX=i-halfWindow\n","            maxX=i+halfWindow+1\n","            minY=j-halfWindow\n","            maxY=j+halfWindow+1\n","            \n","            newImg[a][b][1] = img[minX:maxX,minY:maxY].mean() #meanNIR\n","            newImg[a][b][0] = math.sqrt(np.var(img[minX:maxX,minY:maxY])) #stdNIR\n","            b = (b+1)%478\n","        a += 1\n","    \n","    return newImg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHy5HfHu-HBB","colab_type":"code","colab":{}},"source":["import tifffile as tiff\n","import cv2\n","from shapely.wkt import loads\n","\n","N_Cls=10\n","\n","def all_image_mod(image_id):\n","    imgRGBN = np.zeros((3346, 3346, 4), \"float32\")\n","    # for type M \n","    imgRGBN[..., 3] = cv2.resize(np.transpose(tiff.imread(\"/content/drive/My Drive/GP/dstl/sixteen_band/{}_M.tif\".format(image_id)), (1,2,0))[:,:,7], (3346, 3346))\n","    #img_NIR = cv2.resize(img_NIR, (3346, 3346)) #stretch to fit 7*7\n","\n","    # for RGB\n","    imgRGBN[..., 0:3] = cv2.resize(np.moveaxis(tiff.imread(\"/content/drive/My Drive/GP/dstl/three_band/{}.tif\".format(image_id)),0,-1), (3346, 3346)) #compress to fit 7*7\n","    #img_RGB = img_RGB[:3347,:3347,:] \n","    \n","    #print(f'RGB shape: {img_RGB.shape}\\nM shape: {img_M.shape}\\nimg shape: {img.shape}')\n","    \n","    #imgRGBN = stretch_n(imgRGBN)\n","    imgHSI = convertRGB2HSI(imgRGBN[:,:,0:3])\n","    img = np.zeros((478,478,16)) \n","    \n","    temp = computeCCMFeatures(imgHSI[:,:,0],'H')\n","    print(\"ccmH\")\n","    img[:,:,4]=temp[:,:,0]\n","    img[:,:,1]=temp[:,:,1]\n","    img[:,:,2]=temp[:,:,2]\n","\n","    temp = computeCCMFeatures(imgHSI[:,:,1],'S')\n","    print(\"ccmS\")\n","    img[:,:,3]=temp[:,:,0]\n","    img[:,:,5]=temp[:,:,1]\n","\n","    temp = computeCCMFeatures(imgHSI[:,:,2],'I')\n","    print(\"ccmI\")\n","    img[:,:,0]=temp[:,:,0]\n","    img[:,:,6]=temp[:,:,1]\n","\n","    temp = computeHSIFeatures(imgHSI)\n","    print(\"HSI\")\n","    img[:,:,7:9]=temp[:,:,0:2]\n","    img[:,:,10:15]=temp[:,:,2:7]\n","\n","    temp = computeRGBNFeatures(imgRGBN[...,3])\n","    print(\"RGBN\")\n","    img[:,:,9]=temp[:,:,0]\n","    img[:,:,15]=temp[:,:,1]\n","\n","    return img\n","\n","def stick_all_train_mod():\n","    s = 478 #478 is my image size\n","    x = np.zeros((5 * s, 5 * s, 16))\n","    #x = np.load((chosen_path+'x_trn_%d.npy') % N_Cls)\n","    y = np.zeros((5 * s, 5 * s, N_Cls))\n","\n","    ids = sorted(tr_wkt.ImageId.unique())\n","    print(len(ids))\n","    for i in range(5):\n","        for j in range(5):\n","            if np.amax(x[s * i:s * i + s, s * j:s * j + s, :]) == 0:\n","                id = ids[5 * i + j]\n","                img = all_image_mod(id)\n","                #print(img.shape, id, np.amax(img), np.amin(img))\n","                x[s * i:s * i + s, s * j:s * j + s, :] = img[:s, :s, :]\n","                np.save((chosen_path+'x_trn_%d') % N_Cls, x)\n","                for z in range(N_Cls):\n","                    y[s * i:s * i + s, s * j:s * j + s, z] = generate_mask_for_image_and_class((s, s), id, z + 1)\n","            print(5 * i + j)\n","                        \n","\n","    #print(np.amax(y), np.amin(y))\n","\n","    np.save((chosen_path+'x_trn_%d') % N_Cls, x)\n","    np.save((chosen_path+'y_trn_%d') % N_Cls, y)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yaxeygF2JVNW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1596789664873,"user_tz":-120,"elapsed":35951,"user":{"displayName":"Walid Mohammad Hella","photoUrl":"https://lh3.googleusercontent.com/-MwgtfecJDio/AAAAAAAAAAI/AAAAAAAAAHo/wusiBvv1oRc/s64/photo.jpg","userId":"04532010003537326230"}},"outputId":"d7dfc455-95d0-4b4f-b98b-3e91742f6903"},"source":["'''\n","def computeLastCCMFeature(img,windowSize = 7):\n","    img = np.around(img)  \n","    img = img.astype(np.uint8)\n","  \n","    newImg = np.zeros((478,478))\n","    halfWindow=windowSize//2\n","    ccm = np.zeros((256,256), dtype=np.uint8) #assuming windowSize <= 15\n","    a = 0\n","    b = 0\n","    for i in range(halfWindow,img.shape[0]-halfWindow,windowSize):\n","        for j in range(halfWindow,img.shape[1]-halfWindow,windowSize):\n","            # i,j = pixel in whole-image loop\n","            ccm = np.zeros((256,256), dtype=np.uint8) #assuming windowSize <= 15\n","            for k in range(i-halfWindow,min(img.shape[0]-1,i+halfWindow+1)):\n","                for l in range(j-halfWindow,min(img.shape[1]-1,j+halfWindow+1)):\n","                    # k,l = pixel in sliding window loop\n","                    ccm[img[k,l],img[k+1,l+1]] += 1\n","            \n","            \n","            newImg[a][b]=np.sum(np.square(img[(i-halfWindow):min(img.shape[0]-1,i+halfWindow+1), (j-halfWindow):min(img.shape[1]-1,j+halfWindow+1)]-ccm.mean())) #sosvh\n","            \n","            b = (b+1)%478\n","        a += 1\n","    return newImg\n","def all_image_last(image_id):\n","    imgRGBN = np.zeros((3346, 3346, 3), \"float32\")\n","    # for type M \n","    #imgRGBN[..., 3] = cv2.resize(np.transpose(tiff.imread(\"/content/drive/My Drive/GP/dstl/sixteen_band/{}_M.tif\".format(image_id)), (1,2,0))[:,:,7], (3346, 3346))\n","    #img_NIR = cv2.resize(img_NIR, (3346, 3346)) #stretch to fit 7*7\n","\n","    # for RGB\n","    imgRGBN[..., 0:3] = cv2.resize(np.moveaxis(tiff.imread(\"/content/drive/My Drive/GP/dstl/three_band/{}.tif\".format(image_id)),0,-1), (3346, 3346)) #compress to fit 7*7\n","    #img_RGB = img_RGB[:3347,:3347,:] \n","    \n","    #print(f'RGB shape: {img_RGB.shape}\\nM shape: {img_M.shape}\\nimg shape: {img.shape}')\n","    \n","    img = computeLastCCMFeature(convertRGB2HSI(imgRGBN[:,:,0:3])[:,:,0])\n","\n","    return img\n","\n","def stick_all_train_last():\n","    s = 478\n","    x = np.zeros((5 * s, 5 * s))\n","\n","    ids = sorted(tr_wkt.ImageId.unique())\n","    for i in range(5):\n","        for j in range(5):\n","            #if np.amax(x[s * i:s * i + s, s * j:s * j + s, :]) == 0:\n","            id = ids[5 * i + j]\n","            img = all_image_last(id)\n","            print(img.shape, id, np.amax(img), np.amin(img))\n","            x[s * i:s * i + s, s * j:s * j + s] = img[:s, :s]\n","            np.save((chosen_path+'x_whole_10_478_last'), x)\n","            print(5 * i + j)\n","                            \n","\n","    print(np.amax(x), np.amin(x))\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ndef computeLastCCMFeature(img,windowSize = 7):\\n    img = np.around(img)  \\n    img = img.astype(np.uint8)\\n  \\n    newImg = np.zeros((478,478))\\n    halfWindow=windowSize//2\\n    ccm = np.zeros((256,256), dtype=np.uint8) #assuming windowSize <= 15\\n    a = 0\\n    b = 0\\n    for i in range(halfWindow,img.shape[0]-halfWindow,windowSize):\\n        for j in range(halfWindow,img.shape[1]-halfWindow,windowSize):\\n            # i,j = pixel in whole-image loop\\n            ccm = np.zeros((256,256), dtype=np.uint8) #assuming windowSize <= 15\\n            for k in range(i-halfWindow,min(img.shape[0]-1,i+halfWindow+1)):\\n                for l in range(j-halfWindow,min(img.shape[1]-1,j+halfWindow+1)):\\n                    # k,l = pixel in sliding window loop\\n                    ccm[img[k,l],img[k+1,l+1]] += 1\\n            \\n            \\n            newImg[a][b]=np.sum(np.square(img[(i-halfWindow):min(img.shape[0]-1,i+halfWindow+1), (j-halfWindow):min(img.shape[1]-1,j+halfWindow+1)]-ccm.mean())) #sosvh\\n            \\n            b = (b+1)%478\\n        a += 1\\n    return newImg\\ndef all_image_last(image_id):\\n    imgRGBN = np.zeros((3346, 3346, 3), \"float32\")\\n    # for type M \\n    #imgRGBN[..., 3] = cv2.resize(np.transpose(tiff.imread(\"/content/drive/My Drive/GP/dstl/sixteen_band/{}_M.tif\".format(image_id)), (1,2,0))[:,:,7], (3346, 3346))\\n    #img_NIR = cv2.resize(img_NIR, (3346, 3346)) #stretch to fit 7*7\\n\\n    # for RGB\\n    imgRGBN[..., 0:3] = cv2.resize(np.moveaxis(tiff.imread(\"/content/drive/My Drive/GP/dstl/three_band/{}.tif\".format(image_id)),0,-1), (3346, 3346)) #compress to fit 7*7\\n    #img_RGB = img_RGB[:3347,:3347,:] \\n    \\n    #print(f\\'RGB shape: {img_RGB.shape}\\nM shape: {img_M.shape}\\nimg shape: {img.shape}\\')\\n    \\n    img = computeLastCCMFeature(convertRGB2HSI(imgRGBN[:,:,0:3])[:,:,0])\\n\\n    return img\\n\\ndef stick_all_train_last():\\n    s = 478\\n    x = np.zeros((5 * s, 5 * s))\\n\\n    ids = sorted(tr_wkt.ImageId.unique())\\n    for i in range(5):\\n        for j in range(5):\\n            #if np.amax(x[s * i:s * i + s, s * j:s * j + s, :]) == 0:\\n            id = ids[5 * i + j]\\n            img = all_image_last(id)\\n            print(img.shape, id, np.amax(img), np.amin(img))\\n            x[s * i:s * i + s, s * j:s * j + s] = img[:s, :s]\\n            np.save((chosen_path+\\'x_whole_10_478_last\\'), x)\\n            print(5 * i + j)\\n                            \\n\\n    print(np.amax(x), np.amin(x))\\n'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"Ei32wsE2-15a","colab_type":"code","colab":{}},"source":["chosen_path = \"/content/drive/My Drive/GP/dstl/walid/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4idx5zkNL8Qp","colab_type":"code","colab":{}},"source":["#stick_all_train_last()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0cadcDA_I-Av","colab_type":"code","colab":{}},"source":["#stick_all_train_mod()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9T-cByXjjAr0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1596789664878,"user_tz":-120,"elapsed":35935,"user":{"displayName":"Walid Mohammad Hella","photoUrl":"https://lh3.googleusercontent.com/-MwgtfecJDio/AAAAAAAAAAI/AAAAAAAAAHo/wusiBvv1oRc/s64/photo.jpg","userId":"04532010003537326230"}},"outputId":"421b104b-4b2c-4ea8-e91a-d593afaef190"},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AegYQg_01vk6","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from abc import ABCMeta, abstractmethod\n","from scipy.stats import truncnorm\n","#from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n","\n","def weight_variable(func, shape, stddev, dtype=tf.float32):\n","    initial = func(shape, stddev=stddev, dtype=dtype)\n","    return tf.Variable(initial)\n","\n","def bias_variable(value, shape, dtype=tf.float32):\n","    initial = tf.constant(value, shape=shape, dtype=dtype)\n","    return tf.Variable(initial)\n","\n","def batch_generator(batch_size, data, labels=None):\n","    \"\"\"\n","    Generates batches of samples\n","    :param data: array-like, shape = (n_samples, n_features)\n","    :param labels: array-like, shape = (n_samples, )\n","    :return:\n","    \"\"\"\n","    n_batches = int(np.ceil(len(data) / float(batch_size)))\n","    idx = np.random.permutation(len(data))\n","    data_shuffled = data[idx]\n","    if labels is not None:\n","        labels_shuffled = labels[idx]\n","    for i in range(n_batches):\n","        start = i * batch_size\n","        end = start + batch_size\n","        if labels is not None:\n","            yield data_shuffled[start:end, :], labels_shuffled[start:end]\n","        else:\n","            yield data_shuffled[start:end, :]\n","\n","\n","def to_categorical(labels, num_classes):\n","    \"\"\"\n","    Converts labels as single integer to row vectors. For instance, given a three class problem, labels would be\n","    mapped as label_1: [1 0 0], label_2: [0 1 0], label_3: [0, 0, 1] where labels can be either int or string.\n","    :param labels: array-like, shape = (n_samples, )\n","    :return:\n","    \"\"\"\n","    new_labels = np.zeros([len(labels), num_classes])\n","    label_to_idx_map, idx_to_label_map = dict(), dict()\n","    idx = 0\n","    for i, label in enumerate(labels):\n","        if label not in label_to_idx_map:\n","            label_to_idx_map[label] = idx\n","            idx_to_label_map[idx] = label\n","            idx += 1\n","        new_labels[i][label_to_idx_map[label]] = 1\n","    return new_labels, label_to_idx_map, idx_to_label_map\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k_03UqrZnyld","colab_type":"code","colab":{}},"source":["class BinaryRBM():\n","    \"\"\"\n","    This class implements a Binary Restricted Boltzmann machine.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 n_hidden_units=100,\n","                 activation_function='sigmoid',\n","                 optimization_algorithm='sgd',\n","                 learning_rate=1e-3,\n","                 n_epochs=10,\n","                 contrastive_divergence_iter=1,\n","                 batch_size=32,\n","                 verbose=True):\n","        self.n_hidden_units = n_hidden_units\n","        self.activation_function = activation_function\n","        self.optimization_algorithm = optimization_algorithm\n","        self.learning_rate = learning_rate\n","        self.n_epochs = n_epochs\n","        self.contrastive_divergence_iter = contrastive_divergence_iter\n","        self.batch_size = batch_size\n","        self.verbose = verbose\n","\n","    def fit(self, X):\n","        \"\"\"\n","        Fit a model given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        self.n_visible_units = X.shape[1]\n","\n","        # Initialize RBM parameters\n","        self._build_model()\n","\n","        sess.run(tf.variables_initializer([self.W, self.c, self.b]))\n","\n","        if self.optimization_algorithm == 'sgd':\n","            self._stochastic_gradient_descent(X)\n","        else:\n","            raise ValueError(\"Invalid optimization algorithm.\")\n","        return\n","    \n","    @classmethod\n","    def _get_weight_variables_names(cls):\n","        return ['W', 'c', 'b']\n","\n","    @classmethod\n","    def _get_param_names(cls):\n","        return ['n_hidden_units',\n","                'n_visible_units',\n","                'activation_function',\n","                'optimization_algorithm',\n","                'learning_rate',\n","                'n_epochs',\n","                'contrastive_divergence_iter',\n","                'batch_size',\n","                'verbose',\n","                '_activation_function_class']\n","\n","    def _initialize_weights(self, weights):\n","        if weights:\n","            for attr_name, value in weights.items():\n","                self.__setattr__(attr_name, tf.Variable(value))\n","        else:\n","            if self.activation_function == 'sigmoid':\n","                stddev = 1.0 / np.sqrt(self.n_visible_units)\n","                self.W = weight_variable(tf.random_normal, [self.n_hidden_units, self.n_visible_units], stddev)\n","                self.c = weight_variable(tf.random_normal, [self.n_hidden_units], stddev)\n","                self.b = weight_variable(tf.random_normal, [self.n_visible_units], stddev)\n","                self._activation_function_class = tf.nn.sigmoid\n","            elif self.activation_function == 'relu':\n","                stddev = 0.1 / np.sqrt(self.n_visible_units)\n","                self.W = weight_variable(tf.truncated_normal, [self.n_hidden_units, self.n_visible_units], stddev)\n","                self.c = bias_variable(stddev, [self.n_hidden_units])\n","                self.b = bias_variable(stddev, [self.n_visible_units])\n","                self._activation_function_class = tf.nn.relu\n","            else:\n","                raise ValueError(\"Invalid activation function.\")\n","\n","    def _build_model(self, weights=None):\n","        \"\"\"\n","        Builds TensorFlow model.\n","        :return:\n","        \"\"\"\n","        # initialize weights and biases\n","        self._initialize_weights(weights)\n","\n","        # TensorFlow operations\n","        self.visible_units_placeholder = tf.placeholder(tf.float32, shape=[None, self.n_visible_units])\n","        self.compute_hidden_units_op = self._activation_function_class(\n","            tf.transpose(tf.matmul(self.W, tf.transpose(self.visible_units_placeholder))) + self.c)\n","        self.hidden_units_placeholder = tf.placeholder(tf.float32, shape=[None, self.n_hidden_units])\n","        self.compute_visible_units_op = self._activation_function_class(\n","            tf.matmul(self.hidden_units_placeholder, self.W) + self.b)\n","        self.random_uniform_values = tf.Variable(tf.random_uniform([self.batch_size, self.n_hidden_units]))\n","        sample_hidden_units_op = tf.to_float(self.random_uniform_values < self.compute_hidden_units_op)\n","        self.random_variables = [self.random_uniform_values]\n","\n","        # Positive gradient\n","        # Outer product. N is the batch size length.\n","        # From http://stackoverflow.com/questions/35213787/tensorflow-batch-outer-product\n","        positive_gradient_op = tf.matmul(tf.expand_dims(sample_hidden_units_op, 2),  # [N, U, 1]\n","                                         tf.expand_dims(self.visible_units_placeholder, 1))  # [N, 1, V]\n","\n","        # Negative gradient\n","        # Gibbs sampling\n","        sample_hidden_units_gibbs_step_op = sample_hidden_units_op\n","        for t in range(self.contrastive_divergence_iter):\n","            compute_visible_units_op = self._activation_function_class(\n","                tf.matmul(sample_hidden_units_gibbs_step_op, self.W) + self.b)\n","            compute_hidden_units_gibbs_step_op = self._activation_function_class(\n","                tf.transpose(tf.matmul(self.W, tf.transpose(compute_visible_units_op))) + self.c)\n","            random_uniform_values = tf.Variable(tf.random_uniform([self.batch_size, self.n_hidden_units]))\n","            sample_hidden_units_gibbs_step_op = tf.to_float(random_uniform_values < compute_hidden_units_gibbs_step_op)\n","            self.random_variables.append(random_uniform_values)\n","\n","        negative_gradient_op = tf.matmul(tf.expand_dims(sample_hidden_units_gibbs_step_op, 2),  # [N, U, 1]\n","                                         tf.expand_dims(compute_visible_units_op, 1))  # [N, 1, V]\n","\n","        compute_delta_W = tf.reduce_mean(positive_gradient_op - negative_gradient_op, 0)\n","        compute_delta_b = tf.reduce_mean(self.visible_units_placeholder - compute_visible_units_op, 0)\n","        compute_delta_c = tf.reduce_mean(sample_hidden_units_op - sample_hidden_units_gibbs_step_op, 0)\n","\n","        self.update_W = tf.assign_add(self.W, self.learning_rate * compute_delta_W)\n","        self.update_b = tf.assign_add(self.b, self.learning_rate * compute_delta_b)\n","        self.update_c = tf.assign_add(self.c, self.learning_rate * compute_delta_c)\n","\n","    def to_dict(self):\n","        dct_to_save = {name: self.__getattribute__(name) for name in self._get_param_names()}\n","        dct_to_save.update(\n","            {name: self.__getattribute__(name).eval(sess) for name in self._get_weight_variables_names()})\n","        return dct_to_save\n","\n","    @classmethod\n","    def from_dict(cls, dct_to_load):\n","        weights = {var_name: dct_to_load.pop(var_name) for var_name in cls._get_weight_variables_names()}\n","\n","        _activation_function_class = dct_to_load.pop('_activation_function_class')\n","        n_visible_units = dct_to_load.pop('n_visible_units')\n","\n","        instance = cls(**dct_to_load)\n","        setattr(instance, '_activation_function_class', _activation_function_class)\n","        setattr(instance, 'n_visible_units', n_visible_units)\n","\n","        # Initialize RBM parameters\n","        instance._build_model(weights)\n","        sess.run(tf.variables_initializer([getattr(instance, name) for name in cls._get_weight_variables_names()]))\n","\n","        return instance\n","    \n","    def transform(self, X):\n","        \"\"\"\n","        Transforms data using the fitted model.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        if len(X.shape) == 1:  # It is a single sample\n","            return self._compute_hidden_units(X)\n","        transformed_data = self._compute_hidden_units_matrix(X)\n","        return transformed_data\n","\n","    def _reconstruct(self, transformed_data):\n","        \"\"\"\n","        Reconstruct visible units given the hidden layer output.\n","        :param transformed_data: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return self._compute_visible_units_matrix(transformed_data)\n","\n","    def _stochastic_gradient_descent(self, _data):\n","        \"\"\"\n","        Performs stochastic gradient descend optimization algorithm.\n","        :param _data: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        for iteration in range(1, self.n_epochs + 1):\n","            idx = np.random.permutation(len(_data))\n","            data = _data[idx]\n","            for batch in batch_generator(self.batch_size, data):\n","                if len(batch) < self.batch_size:\n","                    # Pad with zeros\n","                    pad = np.zeros((self.batch_size - batch.shape[0], batch.shape[1]), dtype=batch.dtype)\n","                    batch = np.vstack((batch, pad))\n","                sess.run(tf.variables_initializer(self.random_variables))  # Need to re-sample from uniform distribution\n","                sess.run([self.update_W, self.update_b, self.update_c],\n","                         feed_dict={self.visible_units_placeholder: batch})\n","            if self.verbose:\n","                error = self._compute_reconstruction_error(data)\n","                print(\">> Epoch %d finished \\tRBM Reconstruction error %f\" % (iteration, error))\n","\n","    def _contrastive_divergence(self, vector_visible_units):\n","        \"\"\"\n","        Computes gradients using Contrastive Divergence method.\n","        :param vector_visible_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        v_0 = vector_visible_units\n","        v_t = np.array(v_0)\n","\n","        # Sampling\n","        for t in range(self.contrastive_divergence_iter):\n","            h_t = self._sample_hidden_units(v_t)\n","            v_t = self._compute_visible_units(h_t)\n","\n","        # Computing deltas\n","        v_k = v_t\n","        h_0 = self._compute_hidden_units(v_0)\n","        h_k = self._compute_hidden_units(v_k)\n","        delta_W = np.outer(h_0, v_0) - np.outer(h_k, v_k)\n","        delta_b = v_0 - v_k\n","        delta_c = h_0 - h_k\n","\n","        return delta_W, delta_b, delta_c\n","\n","    def _sample_hidden_units(self, vector_visible_units):\n","        \"\"\"\n","        Computes hidden unit activations by sampling from a binomial distribution.\n","        :param vector_visible_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        hidden_units = self._compute_hidden_units(vector_visible_units)\n","        return (np.random.random_sample(len(hidden_units)) < hidden_units).astype(np.int64)\n","\n","    def _sample_visible_units(self, vector_hidden_units):\n","        \"\"\"\n","        Computes visible unit activations by sampling from a binomial distribution.\n","        :param vector_hidden_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        visible_units = self._compute_visible_units(vector_hidden_units)\n","        return (np.random.random_sample(len(visible_units)) < visible_units).astype(np.int64)\n","\n","    def _compute_hidden_units(self, vector_visible_units):\n","        \"\"\"\n","        Computes hidden unit outputs.\n","        :param vector_visible_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        v = np.expand_dims(vector_visible_units, 0)\n","        h = np.squeeze(self._compute_hidden_units_matrix(v))\n","        return np.array([h]) if not h.shape else h\n","\n","    def _compute_hidden_units_matrix(self, matrix_visible_units):\n","        \"\"\"\n","        Computes hidden unit outputs.\n","        :param matrix_visible_units: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return sess.run(self.compute_hidden_units_op,\n","                        feed_dict={self.visible_units_placeholder: matrix_visible_units})\n","\n","    def _compute_visible_units(self, vector_hidden_units):\n","        \"\"\"\n","        Computes visible (or input) unit outputs.\n","        :param vector_hidden_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        h = np.expand_dims(vector_hidden_units, 0)\n","        v = np.squeeze(self._compute_visible_units_matrix(h))\n","        return np.array([v]) if not v.shape else v\n","\n","    def _compute_visible_units_matrix(self, matrix_hidden_units):\n","        \"\"\"\n","        Computes visible (or input) unit outputs.\n","        :param matrix_hidden_units: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return sess.run(self.compute_visible_units_op,\n","                        feed_dict={self.hidden_units_placeholder: matrix_hidden_units})\n","\n","    def _compute_free_energy(self, vector_visible_units):\n","        \"\"\"\n","        Computes the RBM free energy.\n","        :param vector_visible_units: array-like, shape = (n_features, )\n","        :return:\n","        \"\"\"\n","        v = vector_visible_units\n","        return - np.dot(self.b, v) - np.sum(np.log(1 + np.exp(np.dot(self.W, v) + self.c)))\n","\n","    def _compute_reconstruction_error(self, data):\n","        \"\"\"\n","        Computes the reconstruction error of the data.\n","        :param data: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        data_transformed = self.transform(data)\n","        data_reconstructed = self._reconstruct(data_transformed)\n","        return np.mean(np.sum((data_reconstructed - data) ** 2, 1))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I90YBhTtoI66","colab_type":"code","colab":{}},"source":["class SupervisedDBNClassification():\n","    \"\"\"\n","    Abstract class for supervised Deep Belief Network.\n","    \"\"\"\n","    __metaclass__ = ABCMeta\n","    \n","    def __init__(self,\n","                 hidden_layers_structure=[100, 100],\n","                 activation_function='sigmoid',\n","                 optimization_algorithm='sgd',\n","                 learning_rate=1e-3,\n","                 learning_rate_rbm=1e-3,\n","                 n_iter_backprop=100,\n","                 l2_regularization=1.0,\n","                 n_epochs_rbm=10,\n","                 contrastive_divergence_iter=1,\n","                 rbm_batch_size=512,\n","                 ann_batch_size=32,\n","                 dropout_p=0,  # float between 0 and 1. Fraction of the input units to drop\n","                 verbose=True):\n","        self.hidden_layers_structure = hidden_layers_structure\n","        self.activation_function = activation_function\n","        self.optimization_algorithm = optimization_algorithm\n","        self.learning_rate_rbm = learning_rate_rbm\n","        self.n_epochs_rbm = n_epochs_rbm\n","        self.contrastive_divergence_iter = contrastive_divergence_iter\n","        self.rbm_batch_size = rbm_batch_size\n","        self.rbm_layers = None\n","        self.rbm_class = BinaryRBM\n","        self.n_iter_backprop = n_iter_backprop\n","        self.l2_regularization = l2_regularization\n","        self.learning_rate = learning_rate\n","        self.ann_batch_size = ann_batch_size\n","        self.dropout_p = dropout_p\n","        self.p = 1 - self.dropout_p\n","        self.verbose = verbose\n","\n","    @classmethod\n","    def _get_param_names(cls):\n","        return ['hidden_layers_structure',\n","                'activation_function',\n","                'optimization_algorithm',\n","                'learning_rate_rbm',\n","                'n_epochs_rbm',\n","                'contrastive_divergence_iter',\n","                'rbm_batch_size',\n","                'n_iter_backprop',\n","                'l2_regularization',\n","                'learning_rate',\n","                'ann_batch_size',\n","                'dropout_p',\n","                'verbose',\n","                'label_to_idx_map', \n","                'idx_to_label_map']\n","\n","    @classmethod\n","    def _get_weight_variables_names(cls):\n","        return ['W', 'b']\n","\n","    def _initialize_weights(self, weights):\n","        if weights:\n","            for attr_name, value in weights.items():\n","                self.__setattr__(attr_name, tf.Variable(value))\n","        else:\n","            if self.activation_function == 'sigmoid':\n","                stddev = 1.0 / np.sqrt(self.input_units)\n","                self.W = weight_variable(tf.random_normal, [self.input_units, self.num_classes], stddev)\n","                self.b = weight_variable(tf.random_normal, [self.num_classes], stddev)\n","                self._activation_function_class = tf.nn.sigmoid\n","            elif self.activation_function == 'relu':\n","                stddev = 0.1 / np.sqrt(self.input_units)\n","                self.W = weight_variable(tf.truncated_normal, [self.input_units, self.num_classes], stddev)\n","                self.b = bias_variable(stddev, [self.num_classes])\n","                self._activation_function_class = tf.nn.relu\n","            else:\n","                raise ValueError(\"Invalid activation function.\")\n","\n","    def save(self, save_path):\n","        import pickle\n","\n","        with open(save_path, 'wb') as fp:\n","            pickle.dump(self.to_dict(), fp)\n","\n","    @classmethod\n","    def load(cls, load_path):\n","        import pickle\n","\n","        with open(load_path, 'rb') as fp:\n","            dct_to_load = pickle.load(fp)\n","            return cls.from_dict(dct_to_load)\n","\n","    def to_dict(self):\n","        dct_to_save = {name: self.__getattribute__(name) for name in self._get_param_names()}\n","        dct_to_save.update(\n","            {name: self.__getattribute__(name).eval(sess) for name in self._get_weight_variables_names()})\n","        dct_to_save['rbm_layers'] = [rbm.to_dict() for rbm in self.rbm_layers]\n","        dct_to_save['num_classes'] = self.num_classes\n","        return dct_to_save\n","\n","    @classmethod\n","    def from_dict(cls, dct_to_load):\n","        weights = {var_name: dct_to_load.pop(var_name) for var_name in cls._get_weight_variables_names()}\n","        num_classes = dct_to_load.pop('num_classes')\n","        label_to_idx_map = dct_to_load.pop('label_to_idx_map')\n","        idx_to_label_map = dct_to_load.pop('idx_to_label_map')\n","        rbm_layers = dct_to_load.pop('rbm_layers')\n","        \n","        instance = cls(**dct_to_load)\n","        \n","        setattr(instance, 'rbm_layers', [instance.rbm_class.from_dict(rbm) for rbm in rbm_layers])\n","        setattr(instance, 'num_classes', num_classes)\n","        setattr(instance, 'label_to_idx_map', label_to_idx_map)\n","        setattr(instance, 'idx_to_label_map', idx_to_label_map)\n","        # Initialize RBM parameters\n","        instance._build_model(weights)\n","        sess.run(tf.variables_initializer([getattr(instance, name) for name in cls._get_weight_variables_names()]))\n","        return instance\n","\n","    def _build_model(self, weights=None):\n","        self.visible_units_placeholder = self.rbm_layers[0].visible_units_placeholder\n","        keep_prob = tf.placeholder(tf.float32)\n","        visible_units_placeholder_drop = tf.nn.dropout(self.visible_units_placeholder, keep_prob)\n","        self.keep_prob_placeholders = [keep_prob]\n","\n","        # Define tensorflow operation for a forward pass\n","        rbm_activation = visible_units_placeholder_drop\n","        for rbm in self.rbm_layers:\n","            rbm_activation = rbm._activation_function_class(\n","                tf.transpose(tf.matmul(rbm.W, tf.transpose(rbm_activation))) + rbm.c)\n","            keep_prob = tf.placeholder(tf.float32)\n","            self.keep_prob_placeholders.append(keep_prob)\n","            rbm_activation = tf.nn.dropout(rbm_activation, keep_prob)\n","\n","        self.transform_op = rbm_activation\n","        self.input_units = self.rbm_layers[-1].n_hidden_units\n","\n","        # weights and biases\n","        self._initialize_weights(weights)\n","\n","        if self.optimization_algorithm == 'sgd':\n","            self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n","        else:\n","            raise ValueError(\"Invalid optimization algorithm.\")\n","\n","        # operations\n","        self.y = tf.matmul(self.transform_op, self.W) + self.b\n","        self.y_ = tf.placeholder(tf.float32, shape=[None, self.num_classes])\n","        self.output = tf.nn.softmax(self.y)\n","        self.cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.y, labels=tf.stop_gradient(self.y_)))\n","        self.train_step = self.optimizer.minimize(self.cost_function)\n","\n","    def _fine_tuning(self, data, _labels):\n","        self.num_classes = self._determine_num_output_neurons(_labels)\n","        if self.num_classes == 1:\n","            _labels = np.expand_dims(_labels, -1)\n","\n","        self._build_model()\n","        sess.run(tf.variables_initializer([self.W, self.b]))\n","\n","        labels = self._transform_labels_to_network_format(_labels)\n","\n","        if self.verbose:\n","            print(\"[START] Fine tuning step:\")\n","        self._stochastic_gradient_descent(data, labels)\n","        if self.verbose:\n","            print(\"[END] Fine tuning step\")\n","\n","    def _stochastic_gradient_descent(self, data, labels):\n","        for iteration in range(self.n_iter_backprop):\n","            for batch_data, batch_labels in batch_generator(self.ann_batch_size, data, labels):\n","                feed_dict = {self.visible_units_placeholder: batch_data,\n","                             self.y_: batch_labels}\n","                feed_dict.update({placeholder: self.p for placeholder in self.keep_prob_placeholders})\n","                sess.run(self.train_step, feed_dict=feed_dict)\n","\n","            if self.verbose:\n","                feed_dict = {self.visible_units_placeholder: data, self.y_: labels}\n","                feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n","                error = sess.run(self.cost_function, feed_dict=feed_dict)\n","                print(\">> Epoch %d finished \\tANN training loss %f\" % (iteration, error))\n","    \n","    ###Not used\n","    def transform_dbn(self, X):\n","        \"\"\"\n","        Transforms data using the fitted model.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        input_data = X\n","        for rbm in self.rbm_layers:\n","            input_data = rbm.transform(input_data)\n","        return input_data\n","\n","    ###Not used\n","    def transform_ann(self, X):\n","        feed_dict = {self.visible_units_placeholder: X}\n","        feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n","        return sess.run(self.transform_op,\n","                        feed_dict=feed_dict)\n","\n","    def _compute_output_units_matrix(self, matrix_visible_units):\n","        feed_dict = {self.visible_units_placeholder: matrix_visible_units}\n","        feed_dict.update({placeholder: 1.0 for placeholder in self.keep_prob_placeholders})\n","        return sess.run(self.output, feed_dict=feed_dict)\n","\n","    def fit(self, X, y=None):\n","        \"\"\"\n","        Fits a model given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :param y : array-like, shape = (n_samples, )\n","        :param pre_train: bool\n","        :return:\n","        \"\"\"\n","        self.pre_train(X)\n","        self._fine_tuning(X, y)\n","        return self\n","\n","    def pre_train(self, X):\n","        \"\"\"\n","        Apply unsupervised network pre-training.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        self.rbm_layers = list()\n","        for n_hidden_units in self.hidden_layers_structure:\n","            rbm = self.rbm_class(n_hidden_units=n_hidden_units,\n","                                 activation_function=self.activation_function,\n","                                 optimization_algorithm=self.optimization_algorithm,\n","                                 learning_rate=self.learning_rate_rbm,\n","                                 n_epochs=self.n_epochs_rbm,\n","                                 contrastive_divergence_iter=self.contrastive_divergence_iter,\n","                                 batch_size=self.rbm_batch_size,\n","                                 verbose=self.verbose)\n","            self.rbm_layers.append(rbm)\n","\n","        # Fit RBM\n","        if self.verbose:\n","            print(\"[START] Pre-training step:\")\n","        input_data = X\n","        for rbm in self.rbm_layers:\n","            rbm.fit(input_data)\n","            input_data = rbm.transform(input_data)\n","        if self.verbose:\n","            print(\"[END] Pre-training step\")\n","        return self\n","\n","\n","    def _transform_labels_to_network_format(self, labels):\n","        new_labels, label_to_idx_map, idx_to_label_map = to_categorical(labels, self.num_classes)\n","        self.label_to_idx_map = label_to_idx_map\n","        self.idx_to_label_map = idx_to_label_map\n","        return new_labels\n","\n","    def _transform_network_format_to_labels(self, indexes):\n","        \"\"\"\n","        Converts network output to original labels.\n","        :param indexes: array-like, shape = (n_samples, )\n","        :return:\n","        \"\"\"\n","        return list(map(lambda idx: self.idx_to_label_map[idx], indexes))\n","\n","    def predict(self, X):\n","        probs = self.predict_proba(X)\n","        indexes = np.argmax(probs, axis=1)\n","        return self._transform_network_format_to_labels(indexes)\n","\n","    def predict_proba(self, X):\n","        \"\"\"\n","        Predicts probability distribution of classes for each sample in the given data.\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        return self._compute_output_units_matrix(X)\n","\n","    def predict_proba_dict(self, X):\n","        \"\"\"\n","        Predicts probability distribution of classes for each sample in the given data.\n","        Returns a list of dictionaries, one per sample. Each dict contains {label_1: prob_1, ..., label_j: prob_j}\n","        :param X: array-like, shape = (n_samples, n_features)\n","        :return:\n","        \"\"\"\n","        if len(X.shape) == 1:  # It is a single sample\n","            X = np.expand_dims(X, 0)\n","\n","        predicted_probs = self.predict_proba(X)\n","\n","        result = []\n","        num_of_data, num_of_labels = predicted_probs.shape\n","        for i in range(num_of_data):\n","            # key : label\n","            # value : predicted probability\n","            dict_prob = {}\n","            for j in range(num_of_labels):\n","                dict_prob[self.idx_to_label_map[j]] = predicted_probs[i][j]\n","            result.append(dict_prob)\n","\n","        return result\n","\n","    def _determine_num_output_neurons(self, labels):\n","        return len(np.unique(labels))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCyawoIrypns","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":111},"executionInfo":{"status":"ok","timestamp":1596789671724,"user_tz":-120,"elapsed":42762,"user":{"displayName":"Walid Mohammad Hella","photoUrl":"https://lh3.googleusercontent.com/-MwgtfecJDio/AAAAAAAAAAI/AAAAAAAAAHo/wusiBvv1oRc/s64/photo.jpg","userId":"04532010003537326230"}},"outputId":"c2e2b9fc-ff57-4c5b-cf98-804214752437"},"source":["#examples.py\n","#np.random.seed(1337)  # for reproducibility\n","from time import time\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics.classification import accuracy_score\n","'''\n","X = np.load(('/content/drive/My Drive/GP/dstl/walid/x_whole_10_478_16.npy'))\n","Y = np.load(('/content/drive/My Drive/GP/dstl/walid/y_trn_10.npy'))\n","X = computeNormal3D(X)\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nX = np.load(('/content/drive/My Drive/GP/dstl/walid/x_whole_10_478_16.npy'))\\nY = np.load(('/content/drive/My Drive/GP/dstl/walid/y_trn_10.npy'))\\nX = computeNormal3D(X)\\n\""]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"GNXV4-XNI6TR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1596789671725,"user_tz":-120,"elapsed":42754,"user":{"displayName":"Walid Mohammad Hella","photoUrl":"https://lh3.googleusercontent.com/-MwgtfecJDio/AAAAAAAAAAI/AAAAAAAAAHo/wusiBvv1oRc/s64/photo.jpg","userId":"04532010003537326230"}},"outputId":"1921bd42-3ea7-4a4e-8b43-063888300b69"},"source":["'''Y2 = np.zeros((Y.shape[0],Y.shape[1]))\n","for i in range(Y.shape[0]):\n","    for j in range(Y.shape[1]):\n","        curr = 10\n","        for k in range(Y.shape[2]):\n","            if Y[i,j,k] != 0:\n","                curr = k\n","        Y2[i,j]=curr\n","Y2 = Y2.flatten()'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Y2 = np.zeros((Y.shape[0],Y.shape[1]))\\nfor i in range(Y.shape[0]):\\n    for j in range(Y.shape[1]):\\n        curr = 10\\n        for k in range(Y.shape[2]):\\n            if Y[i,j,k] != 0:\\n                curr = k\\n        Y2[i,j]=curr\\nY2 = Y2.flatten()'"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"kjqSO2jTKWbw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1596789671725,"user_tz":-120,"elapsed":42746,"user":{"displayName":"Walid Mohammad Hella","photoUrl":"https://lh3.googleusercontent.com/-MwgtfecJDio/AAAAAAAAAAI/AAAAAAAAAHo/wusiBvv1oRc/s64/photo.jpg","userId":"04532010003537326230"}},"outputId":"16a9d25c-631e-4468-80f1-1b7bbce37e64"},"source":["'''\n","X2 = np.zeros((X.shape[0]*X.shape[1],X.shape[2]))\n","for i in range(X.shape[0]):\n","    for j in range(X.shape[1]):\n","        X2[i*X.shape[0]+j]=X[i,j]\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nX2 = np.zeros((X.shape[0]*X.shape[1],X.shape[2]))\\nfor i in range(X.shape[0]):\\n    for j in range(X.shape[1]):\\n        X2[i*X.shape[0]+j]=X[i,j]\\n'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"36sGigzjLQLO","colab_type":"code","colab":{}},"source":["#print(X2.shape)\n","#np.save('/content/drive/My Drive/GP/dstl/walid/x_whole_10_478_16_flat',X2)\n","#np.save('/content/drive/My Drive/GP/dstl/walid/CDBN_512/y2_trn_10',Y2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RFxoxIi5pVUm","colab_type":"code","colab":{}},"source":["sess = tf.Session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcdRcBWhFKRh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"executionInfo":{"status":"ok","timestamp":1596789673626,"user_tz":-120,"elapsed":44631,"user":{"displayName":"Walid Mohammad Hella","photoUrl":"https://lh3.googleusercontent.com/-MwgtfecJDio/AAAAAAAAAAI/AAAAAAAAAHo/wusiBvv1oRc/s64/photo.jpg","userId":"04532010003537326230"}},"outputId":"7d987fcb-11db-468e-bd09-211a4583e447"},"source":["'''\n","# Loading dataset\n","digits = load_digits()\n","A, B = digits.data, digits.target\n","\n","# Data scaling\n","A = (A / 16).astype(np.float32)\n","A_train, A_test, B_train, B_test = train_test_split(A, B, test_size=0.2, random_state=0)\n","classifier0 = SupervisedDBNClassification(hidden_layers_structure=[100, 100, 100],\n","                                         learning_rate_rbm=0.05,\n","                                         learning_rate=0.1,\n","                                         n_epochs_rbm=1,\n","                                         n_iter_backprop=25,\n","                                         rbm_batch_size=512,\n","                                         ann_batch_size=512,\n","                                         activation_function='sigmoid',\n","                                         dropout_p=0.2)\n","classifier0.fit(A_train, B_train)\n","B_pred = classifier0.predict(A_test)\n","print('Done.\\nAccuracy: %f' % accuracy_score(B_test, B_pred))\n","classifier0.save('/content/drive/My Drive/GP/dstl/walid/test')\n","classifier0 = SupervisedDBNClassification.load('/content/drive/My Drive/GP/dstl/walid/test')\n","B_pred = classifier0.predict(A_test)\n","print('Done.\\nAccuracy: %f' % accuracy_score(B_test, B_pred))\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n# Loading dataset\\ndigits = load_digits()\\nA, B = digits.data, digits.target\\n\\n# Data scaling\\nA = (A / 16).astype(np.float32)\\nA_train, A_test, B_train, B_test = train_test_split(A, B, test_size=0.2, random_state=0)\\nclassifier0 = SupervisedDBNClassification(hidden_layers_structure=[100, 100, 100],\\n                                         learning_rate_rbm=0.05,\\n                                         learning_rate=0.1,\\n                                         n_epochs_rbm=1,\\n                                         n_iter_backprop=25,\\n                                         rbm_batch_size=512,\\n                                         ann_batch_size=512,\\n                                         activation_function='sigmoid',\\n                                         dropout_p=0.2)\\nclassifier0.fit(A_train, B_train)\\nB_pred = classifier0.predict(A_test)\\nprint('Done.\\nAccuracy: %f' % accuracy_score(B_test, B_pred))\\nclassifier0.save('/content/drive/My Drive/GP/dstl/walid/test')\\nclassifier0 = SupervisedDBNClassification.load('/content/drive/My Drive/GP/dstl/walid/test')\\nB_pred = classifier0.predict(A_test)\\nprint('Done.\\nAccuracy: %f' % accuracy_score(B_test, B_pred))\\n\""]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"bqeQz6d5_raP","colab_type":"code","colab":{}},"source":["# Splitting data\n","X = np.load(('/content/drive/My Drive/GP/dstl/walid/x_whole_10_478_16_flat.npy'))\n","Y = np.load(('/content/drive/My Drive/GP/dstl/walid/y_whole_10_478_flat.npy'))\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.8, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikCwaYO1yS02","colab_type":"code","colab":{}},"source":["# Training\n","classifier = SupervisedDBNClassification(hidden_layers_structure=[100, 100, 100],\n","                                         learning_rate_rbm=0.05,\n","                                         learning_rate=0.1,\n","                                         n_epochs_rbm=3,\n","                                         n_iter_backprop=100,\n","                                         rbm_batch_size=256,\n","                                         ann_batch_size=256,\n","                                         activation_function='sigmoid',\n","                                         dropout_p=0.2)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PCYGbR4hkEoW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596814487823,"user_tz":-120,"elapsed":12173323,"user":{"displayName":"Walid Mohammad Hella","photoUrl":"https://lh3.googleusercontent.com/-MwgtfecJDio/AAAAAAAAAAI/AAAAAAAAAHo/wusiBvv1oRc/s64/photo.jpg","userId":"04532010003537326230"}},"outputId":"08af9d27-4b36-46eb-8bee-9453f6c8428b"},"source":["classifier.fit(X_train, Y_train)\n","classifier.save('/content/drive/My Drive/GP/dstl/walid/model_256batch_256batch_3epochs_20percent')\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[START] Pre-training step:\n","WARNING:tensorflow:From <ipython-input-12-10b26ad7845d>:96: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n",">> Epoch 1 finished \tRBM Reconstruction error 0.099128\n",">> Epoch 2 finished \tRBM Reconstruction error 0.094310\n",">> Epoch 3 finished \tRBM Reconstruction error 0.090825\n",">> Epoch 1 finished \tRBM Reconstruction error 0.079428\n",">> Epoch 2 finished \tRBM Reconstruction error 0.051985\n",">> Epoch 3 finished \tRBM Reconstruction error 0.046091\n",">> Epoch 1 finished \tRBM Reconstruction error 0.034584\n",">> Epoch 2 finished \tRBM Reconstruction error 0.019238\n",">> Epoch 3 finished \tRBM Reconstruction error 0.011648\n","[END] Pre-training step\n","WARNING:tensorflow:From <ipython-input-13-b54409d11880>:122: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","[START] Fine tuning step:\n",">> Epoch 0 finished \tANN training loss 1.370503\n",">> Epoch 1 finished \tANN training loss 1.323756\n",">> Epoch 2 finished \tANN training loss 1.268703\n",">> Epoch 3 finished \tANN training loss 1.240201\n",">> Epoch 4 finished \tANN training loss 1.223378\n",">> Epoch 5 finished \tANN training loss 1.215206\n",">> Epoch 6 finished \tANN training loss 1.215171\n",">> Epoch 7 finished \tANN training loss 1.209589\n",">> Epoch 8 finished \tANN training loss 1.202237\n",">> Epoch 9 finished \tANN training loss 1.199416\n",">> Epoch 10 finished \tANN training loss 1.202246\n",">> Epoch 11 finished \tANN training loss 1.197326\n",">> Epoch 12 finished \tANN training loss 1.196014\n",">> Epoch 13 finished \tANN training loss 1.196321\n",">> Epoch 14 finished \tANN training loss 1.194519\n",">> Epoch 15 finished \tANN training loss 1.191666\n",">> Epoch 16 finished \tANN training loss 1.189707\n",">> Epoch 17 finished \tANN training loss 1.189204\n",">> Epoch 18 finished \tANN training loss 1.186039\n",">> Epoch 19 finished \tANN training loss 1.184430\n",">> Epoch 20 finished \tANN training loss 1.185727\n",">> Epoch 21 finished \tANN training loss 1.180375\n",">> Epoch 22 finished \tANN training loss 1.179002\n",">> Epoch 23 finished \tANN training loss 1.177718\n",">> Epoch 24 finished \tANN training loss 1.173960\n",">> Epoch 25 finished \tANN training loss 1.169927\n",">> Epoch 26 finished \tANN training loss 1.164928\n",">> Epoch 27 finished \tANN training loss 1.163226\n",">> Epoch 28 finished \tANN training loss 1.160950\n",">> Epoch 29 finished \tANN training loss 1.156088\n",">> Epoch 30 finished \tANN training loss 1.150648\n",">> Epoch 31 finished \tANN training loss 1.148036\n",">> Epoch 32 finished \tANN training loss 1.144514\n",">> Epoch 33 finished \tANN training loss 1.139069\n",">> Epoch 34 finished \tANN training loss 1.135739\n",">> Epoch 35 finished \tANN training loss 1.141476\n",">> Epoch 36 finished \tANN training loss 1.136856\n",">> Epoch 37 finished \tANN training loss 1.126778\n",">> Epoch 38 finished \tANN training loss 1.121227\n",">> Epoch 39 finished \tANN training loss 1.120058\n",">> Epoch 40 finished \tANN training loss 1.113182\n",">> Epoch 41 finished \tANN training loss 1.114814\n",">> Epoch 42 finished \tANN training loss 1.112383\n",">> Epoch 43 finished \tANN training loss 1.114110\n",">> Epoch 44 finished \tANN training loss 1.105299\n",">> Epoch 45 finished \tANN training loss 1.097717\n",">> Epoch 46 finished \tANN training loss 1.103588\n",">> Epoch 47 finished \tANN training loss 1.097939\n",">> Epoch 48 finished \tANN training loss 1.097552\n",">> Epoch 49 finished \tANN training loss 1.088885\n",">> Epoch 50 finished \tANN training loss 1.086440\n",">> Epoch 51 finished \tANN training loss 1.087741\n",">> Epoch 52 finished \tANN training loss 1.080022\n",">> Epoch 53 finished \tANN training loss 1.082398\n",">> Epoch 54 finished \tANN training loss 1.081048\n",">> Epoch 55 finished \tANN training loss 1.080060\n",">> Epoch 56 finished \tANN training loss 1.083741\n",">> Epoch 57 finished \tANN training loss 1.072872\n",">> Epoch 58 finished \tANN training loss 1.072904\n",">> Epoch 59 finished \tANN training loss 1.073714\n",">> Epoch 60 finished \tANN training loss 1.075549\n",">> Epoch 61 finished \tANN training loss 1.068271\n",">> Epoch 62 finished \tANN training loss 1.065250\n",">> Epoch 63 finished \tANN training loss 1.067333\n",">> Epoch 64 finished \tANN training loss 1.071340\n",">> Epoch 65 finished \tANN training loss 1.064282\n",">> Epoch 66 finished \tANN training loss 1.071842\n",">> Epoch 67 finished \tANN training loss 1.065957\n",">> Epoch 68 finished \tANN training loss 1.067699\n",">> Epoch 69 finished \tANN training loss 1.066022\n",">> Epoch 70 finished \tANN training loss 1.066201\n",">> Epoch 71 finished \tANN training loss 1.060798\n",">> Epoch 72 finished \tANN training loss 1.067313\n",">> Epoch 73 finished \tANN training loss 1.065900\n",">> Epoch 74 finished \tANN training loss 1.063939\n",">> Epoch 75 finished \tANN training loss 1.058022\n",">> Epoch 76 finished \tANN training loss 1.062715\n",">> Epoch 77 finished \tANN training loss 1.060114\n",">> Epoch 78 finished \tANN training loss 1.069387\n",">> Epoch 79 finished \tANN training loss 1.065190\n",">> Epoch 80 finished \tANN training loss 1.060005\n",">> Epoch 81 finished \tANN training loss 1.065104\n",">> Epoch 82 finished \tANN training loss 1.062289\n",">> Epoch 83 finished \tANN training loss 1.061387\n",">> Epoch 84 finished \tANN training loss 1.059696\n",">> Epoch 85 finished \tANN training loss 1.056781\n",">> Epoch 86 finished \tANN training loss 1.054711\n",">> Epoch 87 finished \tANN training loss 1.059759\n",">> Epoch 88 finished \tANN training loss 1.058380\n",">> Epoch 89 finished \tANN training loss 1.060349\n",">> Epoch 90 finished \tANN training loss 1.055618\n",">> Epoch 91 finished \tANN training loss 1.050947\n",">> Epoch 92 finished \tANN training loss 1.054563\n",">> Epoch 93 finished \tANN training loss 1.063042\n",">> Epoch 94 finished \tANN training loss 1.056031\n",">> Epoch 95 finished \tANN training loss 1.053931\n",">> Epoch 96 finished \tANN training loss 1.054397\n",">> Epoch 97 finished \tANN training loss 1.057968\n",">> Epoch 98 finished \tANN training loss 1.049094\n",">> Epoch 99 finished \tANN training loss 1.059682\n","[END] Fine tuning step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f9nZ45y1hF-U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1596814492043,"user_tz":-120,"elapsed":4232,"user":{"displayName":"Walid Mohammad Hella","photoUrl":"https://lh3.googleusercontent.com/-MwgtfecJDio/AAAAAAAAAAI/AAAAAAAAAHo/wusiBvv1oRc/s64/photo.jpg","userId":"04532010003537326230"}},"outputId":"4cacc7a2-2362-4112-d519-5aa5eff16288"},"source":["# Test\n","Y_pred = classifier.predict(X_test)\n","print('Done.\\nAccuracy with 10 classes: %f' % accuracy_score(Y_test, Y_pred))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Done.\n","Accuracy with 10 classes: 0.609748\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AYxlb2vfun_W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1596814497825,"user_tz":-120,"elapsed":5788,"user":{"displayName":"Walid Mohammad Hella","photoUrl":"https://lh3.googleusercontent.com/-MwgtfecJDio/AAAAAAAAAAI/AAAAAAAAAHo/wusiBvv1oRc/s64/photo.jpg","userId":"04532010003537326230"}},"outputId":"5ec6049d-f329-4fee-d7b0-e5027a329869"},"source":["Y_test2 = np.zeros_like(Y_test)\n","Y_pred2 = np.zeros_like(Y_pred)\n","for i in range(len(Y_test)):\n","    Y_test2[i] = Y_test[i]//2\n","    Y_pred2[i] = Y_pred[i]//2\n","print('Done.\\nAccuracy with 5 classes: %f' % accuracy_score(Y_test2, Y_pred2))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Done.\n","Accuracy with 5 classes: 0.611361\n"],"name":"stdout"}]}]}