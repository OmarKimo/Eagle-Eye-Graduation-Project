{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "\n",
    "\n",
    "def colorPred(num):\n",
    "    assert num in range(0,11)\n",
    "\n",
    "    # buildings & manmade structures\n",
    "    if num < 2: return (255,255,255)\n",
    "    # Road & Track\n",
    "    if num < 4: return (255,255,0)\n",
    "    # Trees & Crops\n",
    "    if num < 6: return (0,255,0)\n",
    "    # Waterway & Standing water\n",
    "    if num < 8: return (0,0, 255)\n",
    "    # Vehicle Large & Small\n",
    "    if num < 10: return (0,255,255)\n",
    "    # unspecified\n",
    "    if num == 10: return (0,0,0)\n",
    "\n",
    "#split data to batches for SGD\n",
    "def batchSplitter(batchSize, x, y=None):\n",
    "    nBatch = int(np.ceil(len(x) / float(batchSize)))\n",
    "    #create a random sequence of indices to 'shuffle' x\n",
    "    permutation = np.random.permutation(len(x))\n",
    "    \n",
    "    if y is None:\n",
    "        xShuffled = x[permutation]\n",
    "        for i in range(nBatch):\n",
    "            first = i * batchSize\n",
    "            last = first + batchSize\n",
    "            yield xShuffled[first:last]\n",
    "    else:\n",
    "        xShuffled = x[permutation]\n",
    "        yShuffled = y[permutation]\n",
    "        for i in range(nBatch):\n",
    "            first = i * batchSize\n",
    "            last = first + batchSize\n",
    "            yield xShuffled[first:last], yShuffled[first:last]\n",
    "\n",
    "\n",
    "def inverseFlatten(pred):\n",
    "    import matplotlib.pyplot as plt\n",
    "    assert type(pred) == list\n",
    "    sz = int(np.sqrt(len(pred)))\n",
    "    assert sz == 478\n",
    "    img = np.zeros((sz,sz,3))\n",
    "    # f = lambda arr: [colorPred(arr[i]) for i in range(len(arr))]\n",
    "    # img = [f(pred[i*sz : (i+1)*sz]) for i in range(sz)]\n",
    "    for i in range(sz):\n",
    "        for j in range(sz):\n",
    "            img[i][j] = colorPred(pred[i*sz+j])\n",
    "    img = cv2.resize(img, (800, 800))\n",
    "    return img\n",
    "def showOriginal(id):\n",
    "    import matplotlib.pyplot as plt\n",
    "    def getImage(image_id, dims=3, size=800, case=\"\"):\n",
    "        filename = \"./testImages/{}.tif\".format(image_id)\n",
    "        img = tiff.imread(filename)\n",
    "        img = np.rollaxis(img, 0, 3)\n",
    "        img = cv2.resize(img, (size, size))\n",
    "        return img\n",
    "    def ContrastAdjustment(img, low_p = 5, high_p = 95):\n",
    "        img_adjusted = np.zeros_like(img, dtype=np.float32)\n",
    "        for i in range(img.shape[2]):\n",
    "            mn, mx = 0, 1       # np.min(img), np.max(img)\n",
    "            p_low, p_high = np.percentile(img[:, :, i], low_p), np.percentile(img[:, :, i], high_p)\n",
    "            tmp = mn + (img[:, :, i] - p_low) * (mx - mn) / (p_high - p_low)\n",
    "            tmp[tmp < mn] = mn\n",
    "            tmp[tmp > mx] = mx\n",
    "            img_adjusted[:, :, i] = tmp\n",
    "        return img_adjusted.astype(np.float32)\n",
    "\n",
    "    img = getImage(id)\n",
    "    img = ContrastAdjustment(img)\n",
    "    plt.figure()\n",
    "    #ax1 = plt.subplot(131)\n",
    "    plt.title('image ID: {}'.format(id))\n",
    "    plt.imshow(img[:, :, :3])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZT5977c95AP"
   },
   "outputs": [],
   "source": [
    "!pip install tifffile\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def computeNormal2D(mat): #input must be a 2D array\n",
    "    amin = np.amin(mat)\n",
    "    mat = (mat - amin) / (np.amax(mat) - amin)\n",
    "    return mat\n",
    "\n",
    "def computeNormal3D(mat): #input must be a 3D array\n",
    "    for i in range(mat.shape[2]):\n",
    "        mat[:,:,i] = computeNormal2D(mat[:,:,i])\n",
    "    return mat\n",
    "\n",
    "def computeSMoment(mat,mean=None): #input must be a 2D array\n",
    "    if mean == None:\n",
    "        mean = mat.mean()\n",
    "    return np.sqrt(mean)/np.square(mat).mean()\n",
    "\n",
    "def convertRGB2HSV(img):\n",
    "    img = img/1023.0\n",
    "    imgHSV = img\n",
    "    for i in range(len(img)):\n",
    "        for j in range(len(img[0])):\n",
    "            cmax = max(img[i][j][0],img[i][j][1],img[i][j][2])\n",
    "            cmin = min(img[i][j][0],img[i][j][1],img[i][j][2])\n",
    "            delta = cmax-cmin\n",
    "\n",
    "            if cmax==cmin:\n",
    "                imgHSV[i][j][0] = 0\n",
    "            elif cmax == img[i][j][0]:\n",
    "                imgHSV[i][j][0] = (60 * ((img[i][j][1] - img[i][j][2]) / delta) + 360) % 360\n",
    "            elif cmax == img[i][j][1]:\n",
    "                imgHSV[i][j][0] = (60 * ((img[i][j][2] - img[i][j][0]) / delta) + 120) % 360\n",
    "            elif cmax == img[i][j][2]:\n",
    "                imgHSV[i][j][0] = (60 * ((img[i][j][0] - img[i][j][1]) / delta) + 240) % 360\n",
    "      \n",
    "            if cmax==0:\n",
    "                imgHSV[i][j][1]=0\n",
    "            else:\n",
    "                imgHSV[i][j][1]=(delta/cmax)\n",
    "      \n",
    "            imgHSV[i][j][2]=cmax\n",
    "\n",
    "    return imgHSV\n",
    "\n",
    "def computeCCMFeatures(img,band,windowSize = 7):\n",
    "\n",
    "    if windowSize % 2 == 0:\n",
    "        return\n",
    "    paramCount = 0\n",
    "    if band == 'H':\n",
    "        paramCount = 3\n",
    "        img = img*255/360\n",
    "    elif band == 'S':\n",
    "        paramCount = 2\n",
    "        img = img*255\n",
    "    elif band == 'V':\n",
    "        paramCount = 2\n",
    "        img = img*255\n",
    "    else:\n",
    "        return\n",
    "  \n",
    "    img = np.around(img)  \n",
    "    img = img.astype(np.uint8)\n",
    "  \n",
    "    newImg = np.zeros((478,478,paramCount))\n",
    "    halfWindow=windowSize//2\n",
    "    ccm = np.zeros((256,256), dtype=np.uint8) #assuming windowSize <= 15\n",
    "    a = 0\n",
    "    b = 0\n",
    "    for i in range(halfWindow,img.shape[0]-halfWindow,windowSize):\n",
    "        for j in range(halfWindow,img.shape[1]-halfWindow,windowSize):\n",
    "            # i,j = pixel in whole-image loop\n",
    "            ccm = np.zeros((256,256), dtype=np.uint8) #assuming windowSize <= 15\n",
    "            for k in range(i-halfWindow,min(img.shape[0]-1,i+halfWindow+1)):\n",
    "                for l in range(j-halfWindow,min(img.shape[1]-1,j+halfWindow+1)):\n",
    "                    # k,l = pixel in window loop\n",
    "                    ccm[img[k,l],img[k+1,l+1]] += 1\n",
    "            mean = ccm.mean()\n",
    "            newImg[a][b][0] = mean\n",
    "            ccmSquared = np.square(ccm)\n",
    "        \n",
    "            if band == 'H':\n",
    "                #sosvh is SSD\n",
    "                newImg[a][b][1]=np.sum(np.square(ccm-mean)) #sosvh\n",
    "                newImg[a][b][2]=np.sum(ccmSquared) #autoc\n",
    "            else:\n",
    "                newImg[a][b][1]=(np.sqrt(mean))/(ccmSquared.mean()) #smoment\n",
    "                #if band == 'V':\n",
    "                #  covariance = np.cov(ccm)[0][-1]\n",
    "                #  newImg[a][b][2]=covariance\n",
    "            b = (b+1)%478\n",
    "        a += 1\n",
    "    return newImg\n",
    "\n",
    "\n",
    "def computeHSVFeatures(img,windowSize = 7):\n",
    "    paramCount = 7\n",
    "    newImg = np.zeros((478,478,paramCount))\n",
    "    halfWindow=windowSize//2\n",
    "    a = 0\n",
    "    b = 0\n",
    "    for i in range(halfWindow,img.shape[0]-halfWindow,7):\n",
    "        for j in range(halfWindow,img.shape[1]-halfWindow,7):\n",
    "            # i,j = pixel in whole-image loop      \n",
    "        \n",
    "            minX=i-halfWindow\n",
    "            maxX=i+halfWindow+1\n",
    "            minY=j-halfWindow\n",
    "            maxY=j+halfWindow+1\n",
    "\n",
    "            newImg[a][b][4] = img[minX:maxX,minY:maxY,0].mean() #meanH\n",
    "            newImg[a][b][6] = img[minX:maxX,minY:maxY,1].mean() #meanS\n",
    "            newImg[a][b][5] = img[minX:maxX,minY:maxY,2].mean() #meanV\n",
    "            newImg[a][b][0] = computeSMoment(img[minX:maxX,minY:maxY,2], newImg[a][b][5]) #smomentV\n",
    "            newImg[a][b][1] = np.var(img[minX:maxX,minY:maxY,2]) #varianceV\n",
    "            newImg[a][b][2] = np.sqrt(newImg[a][b][1]) #stdV\n",
    "            newImg[a][b][3] = np.std(img[minX:maxX,minY:maxY,0]) #stdH\n",
    "            b = (b+1)%478\n",
    "        a += 1\n",
    "        \n",
    "    return newImg\n",
    "\n",
    "def computeNIRFeatures(img,windowSize = 7):\n",
    "    paramCount = 2\n",
    "    newImg = np.zeros((478,478,paramCount))\n",
    "    halfWindow=windowSize//2\n",
    "    a = 0\n",
    "    b = 0\n",
    "    for i in range(halfWindow,img.shape[0]-halfWindow,7):\n",
    "        for j in range(halfWindow,img.shape[1]-halfWindow,7):\n",
    "            # i,j = pixel in whole-image loop      \n",
    "            \n",
    "            minX=i-halfWindow\n",
    "            maxX=i+halfWindow+1\n",
    "            minY=j-halfWindow\n",
    "            maxY=j+halfWindow+1\n",
    "            \n",
    "            newImg[a][b][1] = img[minX:maxX,minY:maxY].mean() #meanNIR\n",
    "            newImg[a][b][0] = np.sqrt(np.var(img[minX:maxX,minY:maxY])) #stdNIR\n",
    "            b = (b+1)%478\n",
    "        a += 1\n",
    "    \n",
    "    return newImg\n",
    "\n",
    "def extractImageFeatures(imagePath):\n",
    "    imgRGBN = np.zeros((3346, 3346, 4), \"float32\")\n",
    "\n",
    "    imgRGBN[..., 3] = cv2.resize(np.transpose(tiff.imread(\"{}_M.tif\".format(imagePath[:-4])), (1,2,0))[:,:,7], (3346, 3346))\n",
    "\n",
    "    imgRGBN[..., 0:3] = cv2.resize(np.moveaxis(tiff.imread(imagePath),0,-1), (3346, 3346)) #compress to fit 7*7\n",
    "\n",
    "    imgHSV = convertRGB2HSV(imgRGBN[:,:,0:3])\n",
    "    img = np.zeros((478,478,16)) \n",
    "    \n",
    "    temp = computeCCMFeatures(imgHSV[:,:,0],'H')\n",
    "    #print(\"ccmH\")\n",
    "    img[:,:,4]=temp[:,:,0]\n",
    "    img[:,:,1]=temp[:,:,1]\n",
    "    img[:,:,2]=temp[:,:,2]\n",
    "\n",
    "    temp = computeCCMFeatures(imgHSV[:,:,1],'S')\n",
    "    #print(\"ccmS\")\n",
    "    img[:,:,3]=temp[:,:,0]\n",
    "    img[:,:,5]=temp[:,:,1]\n",
    "\n",
    "    temp = computeCCMFeatures(imgHSV[:,:,2],'V')\n",
    "    #print(\"ccmI\")\n",
    "    img[:,:,0]=temp[:,:,0]\n",
    "    img[:,:,6]=temp[:,:,1]\n",
    "\n",
    "    temp = computeHSVFeatures(imgHSV)\n",
    "    #print(\"HSV\")\n",
    "    img[:,:,7:9]=temp[:,:,0:2]\n",
    "    img[:,:,10:15]=temp[:,:,2:7]\n",
    "\n",
    "    temp = computeNIRFeatures(imgRGBN[...,3])\n",
    "    #print(\"NIR\")\n",
    "    img[:,:,9]=temp[:,:,0]\n",
    "    img[:,:,15]=temp[:,:,1]\n",
    "\n",
    "    return img\n",
    "\n",
    "def flattenImage(mat): #input must be a 3D array\n",
    "    flatMat = np.zeros((mat.shape[0]*mat.shape[1],mat.shape[2]))\n",
    "    for i in range(mat.shape[0]):\n",
    "        for j in range(mat.shape[1]):\n",
    "            flatMat[i*mat.shape[0]+j]=mat[i,j]\n",
    "    return flatMat\n",
    "\n",
    "def readImageDBN(imagePath):\n",
    "    img = extractImageFeatures(imagePath)\n",
    "    img = computeNormal3D(img)\n",
    "    img = flattenImage(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_path = \"/content/drive/My Drive/GP/dstl/walid/\"\n",
    "\n",
    "def concatFeatureVectors():\n",
    "    s = 478 #478 is my image size\n",
    "    x = np.zeros((5 * s, 5 * s, 16))\n",
    "    #x = np.load((chosen_path+'x_trn_10.npy'))\n",
    "\n",
    "    ids = sorted(tr_wkt.ImageId.unique())\n",
    "    print(len(ids))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            if np.amax(x[s * i:s * i + s, s * j:s * j + s, :]) == 0:\n",
    "                id = ids[5 * i + j]\n",
    "                img = extractImageFeatures(id)\n",
    "                \n",
    "                x[s * i:s * i + s, s * j:s * j + s, :] = img[:s, :s, :]\n",
    "                np.save((chosen_path+'x_trn_10'), x)\n",
    "            print(5 * i + j)\n",
    "            \n",
    "    np.save((chosen_path+'x_trn_10'), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IHy5HfHu-HBB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "!pip install tensorflow==2.0.0\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "class RBM():\n",
    "    def __init__(self,\n",
    "                 nHidden=200,\n",
    "                 learningRate=1e-3,\n",
    "                 nEpochs=5,\n",
    "                 nIterCD=1,\n",
    "                 batchSize=256,\n",
    "                 verbose=True):\n",
    "        self.nHidden = nHidden\n",
    "        self.learningRate = learningRate\n",
    "        self.nEpochs = nEpochs\n",
    "        self.nIterCD = nIterCD\n",
    "        self.batchSize = batchSize\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @classmethod\n",
    "    def variableNames(cls):\n",
    "        return ['nHidden',\n",
    "                'nVisible',\n",
    "                'learningRate',\n",
    "                'nEpochs',\n",
    "                'nIterCD',\n",
    "                'batchSize',\n",
    "                'verbose']\n",
    "\n",
    "    # Inititializes an object with values loaded from a file\n",
    "    @classmethod\n",
    "    def initFromDict(cls, externalDict):\n",
    "        weights = {name: externalDict.pop(name) for name in ['W', 'b', 'a']}\n",
    "\n",
    "        nVisible = externalDict.pop('nVisible')\n",
    "        newObject = cls(**externalDict)\n",
    "        setattr(newObject, 'nVisible', nVisible)\n",
    "\n",
    "        # Initialize RBM\n",
    "        newObject.defineModel(weights)\n",
    "        sess.run(tf.variables_initializer([getattr(newObject, name) for name in ['W', 'b', 'a']]))\n",
    "\n",
    "        return newObject\n",
    "\n",
    "    # Gets the object's variables to save them in a file\n",
    "    def saveToDict(self):\n",
    "        internalDict = {name: self.__getattribute__(name) for name in self.variableNames()}\n",
    "        internalDict.update({name: self.__getattribute__(name).eval(sess) for name in ['W', 'b', 'a']})\n",
    "        return internalDict\n",
    "\n",
    "    def defineModel(self, weights=None):\n",
    "        # Initialize the weights and biases\n",
    "        if weights:\n",
    "            for attr_name, value in weights.items():\n",
    "                self.__setattr__(attr_name, tf.Variable(value))\n",
    "        else:            \n",
    "            std = 1.0 / np.sqrt(self.nVisible)\n",
    "            self.W = tf.Variable(tf.random_normal([self.nHidden, self.nVisible], std))\n",
    "            self.b = tf.Variable(tf.random_normal([self.nHidden], std))\n",
    "            self.a = tf.Variable(tf.random_normal([self.nVisible], std))\n",
    "\n",
    "        # TensorFlow operations\n",
    "        # Assign the visible nodes to a placeholder\n",
    "        self.nodesVisible = tf.placeholder(tf.float32, shape=[None, self.nVisible])\n",
    "        # Forward Pass - Calculate the hidden nodes: H0 = sigmoid(((W x V^T) + c|b)^T)\n",
    "        self.calcH0 = tf.nn.sigmoid(tf.transpose(tf.matmul(self.W, tf.transpose(self.nodesVisible))) + self.b)\n",
    "        # Assign the hidden nodes to a placeholder \n",
    "        self.nodesHidden = tf.placeholder(tf.float32, shape=[None, self.nHidden])\n",
    "        # Backward Pass - Approximate the visible nodes: V' = sigmoid((H x W)+ b|a)\n",
    "        self.calcV0 = tf.nn.sigmoid(tf.matmul(self.nodesHidden, self.W) + self.a)\n",
    "        # Create a matrix RUV0 like H with normaly distributed random variables\n",
    "        RUV = tf.Variable(tf.random_uniform([self.batchSize, self.nHidden]))\n",
    "        # Choose sample values Hs0 from H0 using the RUV0\n",
    "        sampleH0 = tf.cast(RUV < self.calcH0, 'float32')\n",
    "        # append the RUV0 to a list of all RUVs\n",
    "        self.randomVariables = [RUV]\n",
    "\n",
    "        # we multiply the sample hidden values Hs0 by the visible values V\n",
    "        # [B, H, 1] x [B, 1, V] = [B, H, V]\n",
    "        positiveGradient = tf.matmul(tf.expand_dims(sampleH0, 2), tf.expand_dims(self.nodesVisible, 1))\n",
    "\n",
    "        # Negative gradient\n",
    "        # Gibbs sampling\n",
    "        sampleHi = sampleH0\n",
    "        for i in range(self.nIterCD):\n",
    "            # V1 = sig Hs0 x W + a\n",
    "            calcVi = tf.nn.sigmoid(tf.matmul(sampleHi, self.W) + self.a)\n",
    "            # Hi = sigmoid(((W x V1^T) + b)^T)\n",
    "            calcHi = tf.nn.sigmoid(tf.transpose(tf.matmul(self.W, tf.transpose(calcVi))) + self.b)\n",
    "            # create RUVi\n",
    "            RUV = tf.Variable(tf.random_uniform([self.batchSize, self.nHidden]))\n",
    "            # Choose sample values Hsi from H1 using the RUVi\n",
    "            sampleHi = tf.cast(RUV < calcHi, 'float32')\n",
    "            # append the RUVi\n",
    "            self.randomVariables.append(RUV)\n",
    "        \n",
    "        # [B, H, 1] x [B, 1, V] = [B, H, V]\n",
    "        negativeGradient = tf.matmul(tf.expand_dims(sampleHi, 2), tf.expand_dims(calcVi, 1))\n",
    "\n",
    "        # dW = batchAverage(Hs0 x V0 - Hs1 x V1)\n",
    "        calcDeltaW = tf.reduce_mean(positiveGradient - negativeGradient, 0)\n",
    "        # da = batchAverage(V-V1)\n",
    "        calcDeltaA = tf.reduce_mean(self.nodesVisible - calcVi, 0)\n",
    "        # db = batchAverage(Hs0-Hs1)\n",
    "        calcDeltaB = tf.reduce_mean(sampleH0 - sampleHi, 0)\n",
    "\n",
    "        # W = W + r*dW\n",
    "        self.iterateW = tf.assign_add(self.W, self.learningRate * calcDeltaW)\n",
    "        # a = a + r*da\n",
    "        self.iterateA = tf.assign_add(self.a, self.learningRate * calcDeltaA)\n",
    "        # b = b + r*db\n",
    "        self.iterateB = tf.assign_add(self.b, self.learningRate * calcDeltaB)\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.nVisible = X.shape[1]\n",
    "        self.defineModel()\n",
    "        sess.run(tf.variables_initializer([self.W, self.b, self.a]))  \n",
    "\n",
    "        for i in range(self.nEpochs):\n",
    "            idx = np.random.permutation(len(X))\n",
    "            data = X[idx]\n",
    "            for batch in batchSplitter(self.batchSize, data):\n",
    "                if len(batch) < self.batchSize:\n",
    "                    # Zero Padding\n",
    "                    pad = np.zeros((self.batchSize - batch.shape[0], batch.shape[1]), dtype=batch.dtype)\n",
    "                    batch = np.vstack((batch, pad))\n",
    "                # Get new random variables\n",
    "                sess.run(tf.variables_initializer(self.randomVariables))\n",
    "                sess.run([self.iterateW, self.iterateA, self.iterateB],\n",
    "                         feed_dict={self.nodesVisible: batch})\n",
    "            if self.verbose:\n",
    "                print(\"RBM Epoch\", i, \"finished.\")\n",
    "        return\n",
    "    \n",
    "    # make a forward pass\n",
    "    def forwardPass(self, X):\n",
    "        return sess.run(self.calcH0, feed_dict={self.nodesVisible: X})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN():\n",
    "    def __init__(self,\n",
    "                 architectureRBM,\n",
    "                 learningRateNN,\n",
    "                 learningRateRBM,\n",
    "                 nEpochsNN,\n",
    "                 nEpochsRBM,\n",
    "                 nIterCD,\n",
    "                 batchSizeRBM,\n",
    "                 batchSizeNN,\n",
    "                 dropout,\n",
    "                 verbose=True):\n",
    "        self.architectureRBM = architectureRBM\n",
    "        self.learningRateNN = learningRateNN\n",
    "        self.learningRateRBM = learningRateRBM\n",
    "        self.nEpochsNN = nEpochsNN\n",
    "        self.nEpochsRBM = nEpochsRBM\n",
    "        self.nIterCD = nIterCD\n",
    "        self.batchSizeRBM = batchSizeRBM\n",
    "        self.batchSizeNN = batchSizeNN\n",
    "        self.dropout = dropout\n",
    "        self.verbose = verbose\n",
    "        self.stackedRBMs = None\n",
    "\n",
    "    @classmethod\n",
    "    def variableNames(cls):\n",
    "        return ['architectureRBM',\n",
    "                'learningRateRBM',\n",
    "                'nEpochsRBM',\n",
    "                'nIterCD',\n",
    "                'batchSizeRBM',\n",
    "                'nEpochsNN',\n",
    "                'learningRateNN',\n",
    "                'batchSizeNN',\n",
    "                'dropout',\n",
    "                'verbose',\n",
    "                'mapLabel2Index', \n",
    "                'mapIndex2Label']\n",
    "\n",
    "    def save(self, save_path):\n",
    "        import pickle\n",
    "        with open(save_path, 'wb') as filePath:\n",
    "            internalDict = {name: self.__getattribute__(name) for name in self.variableNames()}\n",
    "            internalDict.update({name: self.__getattribute__(name).eval(sess) for name in ['W', 'b']})\n",
    "            internalDict['stackedRBMs'] = [rbm.saveToDict() for rbm in self.stackedRBMs]\n",
    "            internalDict['nClasses'] = self.nClasses\n",
    "            pickle.dump(internalDict, filePath)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, load_path):\n",
    "        import pickle\n",
    "        with open(load_path, 'rb') as filePath:\n",
    "            externalDict = pickle.load(filePath)\n",
    "            weights = {var_name: externalDict.pop(var_name) for var_name in ['W', 'b']}\n",
    "            nClasses = externalDict.pop('nClasses')\n",
    "            mapLabel2Index = externalDict.pop('mapLabel2Index')\n",
    "            mapIndex2Label = externalDict.pop('mapIndex2Label')\n",
    "            stackedRBMs = externalDict.pop('stackedRBMs')\n",
    "            \n",
    "            newObject = cls(**externalDict)\n",
    "            \n",
    "            setattr(newObject, 'stackedRBMs', [RBM.initFromDict(rbm) for rbm in stackedRBMs])\n",
    "            setattr(newObject, 'nClasses', nClasses)\n",
    "            setattr(newObject, 'mapLabel2Index', mapLabel2Index)\n",
    "            setattr(newObject, 'mapIndex2Label', mapIndex2Label)\n",
    "            # Initialize RBM parameters\n",
    "            newObject.defineModel(weights)\n",
    "            sess.run(tf.variables_initializer([getattr(newObject, name) for name in ['W', 'b']]))\n",
    "            return newObject\n",
    "\n",
    "    # convert class label to mask vector.\n",
    "    def mapLabels(self, labels, nClasses):\n",
    "        newLabels = np.zeros([len(labels), nClasses])\n",
    "        mapLabel2Index, mapIndex2Label = dict(), dict()\n",
    "        index = 0\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in mapLabel2Index:\n",
    "                mapIndex2Label[index] = label\n",
    "                mapLabel2Index[label] = index\n",
    "                index += 1\n",
    "            newLabels[i][mapLabel2Index[label]] = 1\n",
    "        return newLabels, mapLabel2Index, mapIndex2Label\n",
    "\n",
    "    def defineModel(self, weights=None):\n",
    "        self.nodesVisible = self.stackedRBMs[0].nodesVisible\n",
    "        keepProb = tf.placeholder(tf.float32)\n",
    "        # Apply dropout on the visible nodes\n",
    "        nodesVisible_drop = tf.nn.dropout(self.nodesVisible, keepProb)\n",
    "        self.keepProbs = [keepProb]\n",
    "\n",
    "        # Define tensorflow operation for a forward pass\n",
    "        self.outputRBM = nodesVisible_drop\n",
    "        for rbm in self.stackedRBMs:\n",
    "            self.outputRBM = tf.nn.sigmoid(tf.transpose(tf.matmul(rbm.W, tf.transpose(self.outputRBM))) + rbm.b)\n",
    "            keepProb = tf.placeholder(tf.float32)\n",
    "            self.keepProbs.append(keepProb)\n",
    "            self.outputRBM = tf.nn.dropout(self.outputRBM, keepProb)\n",
    "\n",
    "        # should be n_nInputNN\n",
    "        self.nInputNN = self.stackedRBMs[-1].nHidden\n",
    "\n",
    "        # Initialize the weights and biases\n",
    "        if weights:\n",
    "            for attr_name, value in weights.items():\n",
    "                self.__setattr__(attr_name, tf.Variable(value))\n",
    "        else:\n",
    "            std = 1.0 / np.sqrt(self.nInputNN)\n",
    "            self.W = tf.Variable(tf.random_normal([self.nInputNN, self.nClasses], std))\n",
    "            self.b = tf.Variable(tf.random_normal([self.nClasses], std))\n",
    "        \n",
    "        # Use Stochastic Gradient Descent optimizer and assign the learning rate\n",
    "        self.optimizerSGD = tf.train.GradientDescentOptimizer(self.learningRateNN)\n",
    "\n",
    "        # operations\n",
    "        self.trueY = tf.placeholder(tf.float32, shape=[None, self.nClasses])\n",
    "        self.predictedY = tf.matmul(self.outputRBM, self.W) + self.b\n",
    "        self.outputNN = tf.nn.softmax(self.predictedY)\n",
    "        self.lossFunction = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(tf.stop_gradient(self.trueY), self.predictedY))\n",
    "        self.trainingStep = self.optimizerSGD.minimize(self.lossFunction)\n",
    "\n",
    "    def fit(self, X, Y=None):\n",
    "        #self.pre_train(X)\n",
    "        self.stackedRBMs = list()\n",
    "        for nHidden in self.architectureRBM:\n",
    "            rbm = RBM(nHidden=nHidden,\n",
    "                            learningRate=self.learningRateRBM,\n",
    "                            nEpochs=self.nEpochsRBM,\n",
    "                            nIterCD=self.nIterCD,\n",
    "                            batchSize=self.batchSizeRBM,\n",
    "                            verbose=self.verbose)\n",
    "            self.stackedRBMs.append(rbm)\n",
    "\n",
    "        # Fit RBM\n",
    "        if self.verbose:\n",
    "            print(\"Unsupervised Learning Phase:\")\n",
    "        inputDataNN = X\n",
    "        for rbm in self.stackedRBMs:\n",
    "            rbm.fit(inputDataNN)\n",
    "            inputDataNN = rbm.forwardPass(inputDataNN)\n",
    "\n",
    "        # Assign the number of nodes(classes)\n",
    "        self.nClasses = len(np.unique(Y))\n",
    "        if self.nClasses == 1:\n",
    "            Y = np.expand_dims(Y, -1)\n",
    "\n",
    "        # Build the neural network\n",
    "        self.defineModel()\n",
    "        sess.run(tf.variables_initializer([self.W, self.b]))\n",
    "\n",
    "        # Change given labels to classifier format\n",
    "        Y, mapLabel2Index, mapIndex2Label = self.mapLabels(Y, self.nClasses)\n",
    "        self.mapLabel2Index = mapLabel2Index\n",
    "        self.mapIndex2Label = mapIndex2Label\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Supervised Learning Phase:\")\n",
    "        for epoch in range(self.nEpochsNN):\n",
    "            for batchData, batchLabels in batchSplitter(self.batchSizeNN, X, Y):\n",
    "                feed_dict = {self.nodesVisible: batchData, self.trueY: batchLabels}\n",
    "                feed_dict.update({placeholder: (1 - self.dropout) for placeholder in self.keepProbs})\n",
    "                sess.run(self.trainingStep, feed_dict=feed_dict)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"NN Epoch %d finished.\" % (epoch))\n",
    "        if self.verbose:\n",
    "            print(\"Model finished.\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict probability of each classes for every datapoint\n",
    "        feed_dict = {self.nodesVisible: X}\n",
    "        feed_dict.update({placeholder: 1.0 for placeholder in self.keepProbs})\n",
    "        probs = sess.run(self.outputNN, feed_dict=feed_dict)\n",
    "        indexes = np.argmax(probs, axis=1)\n",
    "        # Change network output to given labels\n",
    "        labels = map(lambda idx: self.mapIndex2Label[idx], indexes)\n",
    "        return list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X = np.load(('/content/drive/My Drive/GP/dstl/walid/x_whole_10_478_16.npy'))\n",
    "Xnew = np.load(('/content/drive/My Drive/GP/dstl/walid/x_whole_10_478_last.npy'))\n",
    "X[...,1]=Xnew\n",
    "#Y = np.load(('/content/drive/My Drive/GP/dstl/walid/y_trn_10.npy'))\n",
    "X = computeNormal3D(X)\n",
    "Y2 = np.zeros((Y.shape[0],Y.shape[1]))\n",
    "for i in range(Y.shape[0]):\n",
    "    for j in range(Y.shape[1]):\n",
    "        curr = 10\n",
    "        for k in range(Y.shape[2]):\n",
    "            if Y[i,j,k] != 0:\n",
    "                curr = k\n",
    "        Y2[i,j]=curr\n",
    "Y2 = Y2.flatten()''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X2.shape)\n",
    "#np.save('/content/drive/My Drive/GP/dstl/walid/x_whole_10_478_16_flat',X2)\n",
    "#np.save('/content/drive/My Drive/GP/dstl/walid/CDBN_512/y2_trn_10',Y2)\n",
    "#np.save('/content/drive/My Drive/GP/dstl/walid/x_whole_10_478_16',X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building a model using the labeled dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "X = np.load(('/content/drive/My Drive/GP/dstl/walid/x_whole_10_478_16_flat.npy'))\n",
    "Y = np.load(('/content/drive/My Drive/GP/dstl/walid/y_whole_10_478_flat.npy'))\n",
    "X_trn, X_val, Y_trn, Y_val = train_test_split(X, Y, test_size=0.75, random_state=0)\n",
    "# Training\n",
    "classifier = DBN(architectureRBM=[100, 100, 100],\n",
    "                                         learningRateRBM=0.05,\n",
    "                                         learningRateNN=0.1,\n",
    "                                         nEpochsRBM=3,\n",
    "                                         nEpochsNN=100,\n",
    "                                         nIterCD = 1,\n",
    "                                         batchSizeRBM=512,\n",
    "                                         batchSizeNN=64,\n",
    "                                         dropout=0.2)\n",
    "classifier.fit(X_trn, Y_trn)\n",
    "classifier.save('/content/drive/My Drive/GP/dstl/walid/model_512_64_3_25.pk1')\n",
    "# Test\n",
    "Y_pred = classifier.predict(X_val)\n",
    "print('Done. Accuracy with 10 classes: %f' % accuracy_score(Y_val, Y_prd))\n",
    "Y_test2 = np.zeros_like(Y_val)\n",
    "Y_pred2 = np.zeros_like(Y_prd)\n",
    "for i in range(len(Y_test)):\n",
    "    Y_val2[i] = Y_val[i]//2\n",
    "    Y_prd2[i] = Y_prd[i]//2\n",
    "print('Done. Accuracy with 5 classes: %f' % accuracy_score(Y_val2, Y_prd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import jaccard_similarity_score    # actually the same as accuracy, doesn't need 'average' argument\n",
    "\n",
    "classifier = DBN.load('/content/drive/My Drive/GP/dstl/walid/model_512_64_3_25.pk1')\n",
    "# Splitting data\n",
    "X = np.load(('/content/drive/My Drive/GP/dstl/walid/x_whole_10_478_16_flat.npy'))\n",
    "Y = np.load(('/content/drive/My Drive/GP/dstl/walid/y_whole_10_478_flat.npy'))\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.75, random_state=0)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "print('Done. Jaccard Score with 10 classes: %f' % jaccard_score(Y_test, Y_pred, average=\"weighted\"))\n",
    "Y_test2 = np.zeros_like(Y_test)\n",
    "Y_pred2 = np.zeros_like(Y_pred)\n",
    "for i in range(len(Y_test)):\n",
    "    Y_test2[i] = Y_test[i]//2\n",
    "    Y_pred2[i] = Y_pred[i]//2\n",
    "print('Done. Jaccard Score with 5 classes: %f' % jaccard_score(Y_test2, Y_pred2, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = './testImages/6160_0_0.tif'\n",
    "classifier = DBN.load('./savedModels/model_512_64_3_25.pk1')\n",
    "#X = readImageDBN(img)\n",
    "X = np.load('./savedFeatures/6160_0_0_FE.npy')\n",
    "Y = classifier.predict(X)\n",
    "colored_mask = inverseFlatten(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showOriginal('6160_0_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(colored_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "dstl_walid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
